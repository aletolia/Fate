{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a64d08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import MultiTaskModelWithPerTaskFusion\n",
    "from config import MoEConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e70e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialize model and config\n",
    "config = MoEConfig()\n",
    "model = MultiTaskModelWithPerTaskFusion(config=config)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "# 2. Create dummy input tensors\n",
    "batch_size = 1\n",
    "img_features = torch.randn(batch_size, 10, 768)  # (B, L_i, D)\n",
    "text_feats = torch.randn(batch_size, 1, 768)    # (B, L_t, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6262e761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiTaskModelWithPerTaskFusion(\n",
       "  (cross_attn_layers): ModuleList(\n",
       "    (0): Fate(\n",
       "      (img_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (text_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (dropout): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (shared_fusion_module): MultiScaleLatentQueryFusion(\n",
       "    (queries): ParameterList(\n",
       "        (0): Parameter containing: [torch.float32 of size 1x64x768]\n",
       "        (1): Parameter containing: [torch.float32 of size 1x128x768]\n",
       "        (2): Parameter containing: [torch.float32 of size 1x256x768]\n",
       "    )\n",
       "    (fusion_layers): ModuleList(\n",
       "      (0-2): 3 x ModuleDict(\n",
       "        (cross_attn): CrossAttentionLayer(\n",
       "          (norm_q): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm_kv): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (resid_dropout): Dropout(p=0.3, inplace=False)\n",
       "        )\n",
       "        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mtan_attention_layers): ModuleList(\n",
       "    (0-3): 4 x TaskSpecificAttentionLayer(\n",
       "      (attention_net): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=384, bias=True)\n",
       "        (1): ReLU()\n",
       "        (2): Linear(in_features=384, out_features=768, bias=True)\n",
       "        (3): Sigmoid()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_fusion_moe): HierarchicalTaskMoE(\n",
       "    (task_router): Linear(in_features=768, out_features=32, bias=False)\n",
       "    (token_router): TokenRouterGating()\n",
       "    (task_emb): Embedding(4, 768)\n",
       "    (experts): ModuleList(\n",
       "      (0-31): 32 x DeepseekMLP(\n",
       "        (gate_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (up_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (down_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (act_fn): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (task_classifiers): ModuleList(\n",
       "    (0-3): 4 x Linear(in_features=344064, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
