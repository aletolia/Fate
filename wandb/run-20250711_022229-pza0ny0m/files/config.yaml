_wandb:
    value:
        cli_version: 0.21.0
        e:
            zhfzja6263yj48nurwxnrpencvari9r7:
                codePath: main.py
                codePathLocal: main.py
                cpu_count: 16
                cpu_count_logical: 32
                cudaVersion: "12.3"
                disk:
                    /:
                        total: "3697969111040"
                        used: "2695522279424"
                email: 1290279498sytzz@gmail.com
                executable: /home/aletolia/anaconda3/envs/fate/bin/python
                git:
                    commit: 8b3bf53fcfd14d3c39eba7f5cb4362fa519ccd65
                    remote: https://github.com/aletolia/Fate.git
                gpu: NVIDIA GeForce RTX 4090 Laptop GPU
                gpu_count: 1
                gpu_nvidia:
                    - architecture: Ada
                      memoryTotal: "17171480576"
                      name: NVIDIA GeForce RTX 4090 Laptop GPU
                      uuid: GPU-ad82f0e4-caf3-2fc8-60f3-531b10042853
                host: OneLastKiss
                memory:
                    total: "67222200320"
                os: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35
                program: /home/aletolia/documents/CPath/Fate/main.py
                python: CPython 3.10.18
                root: /home/aletolia/documents/CPath/Fate
                startedAt: "2025-07-10T18:22:29.353615Z"
                writerId: zhfzja6263yj48nurwxnrpencvari9r7
        m: []
        python_version: 3.10.18
        t:
            "1":
                - 1
                - 5
                - 11
                - 49
                - 51
                - 53
            "2":
                - 1
                - 5
                - 11
                - 41
                - 49
                - 51
                - 53
                - 63
            "3":
                - 16
            "4": 3.10.18
            "5": 0.21.0
            "6": 4.53.1
            "12": 0.21.0
            "13": linux-x86_64
batch_size:
    value: 1
diversity_beta:
    value: 0.0001
dwa_temperature:
    value: 2
early_stopping_delta:
    value: 0.0001
early_stopping_patience:
    value: 10
folds_dir:
    value: folds_new
learning_rate:
    value: 0.0001
loss_weighting_strategy:
    value: mgda
num_epochs:
    value: 30
output_dir:
    value: ./training_output
seed:
    value: 42
use_diversity_reg:
    value: false
use_pcgrad:
    value: false
use_sampler:
    value: false
use_task_norm:
    value: false
wandb_project:
    value: multitask_moe_refactored
wandb_run_name:
    value: null
weight_decay:
    value: 0.0001
